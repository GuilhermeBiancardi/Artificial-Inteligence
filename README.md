# Artificial-Intelligence
 Informational repository on learning and AI-related Systems.

## Deep Neural Network:

 * [Neural Network in PHP.](https://github.com/GuilhermeBiancardi/Neural-Network-PHP): A aprendizagem profunda, do inglês Deep Learning (também conhecida como aprendizado estruturado profundo, aprendizado hierárquico ou aprendizado de máquina profundo) é um ramo de aprendizado de máquina (Machine Learning) baseado em um conjunto de algoritmos que tentam modelar abstrações de alto nível de dados usando um grafo profundo com várias camadas de processamento, compostas de várias transformações lineares e não lineares.

## Classification Methods

 * [Navy Bayes in PHP.](https://github.com/GuilhermeBiancardi/NavyBayes-class): Em estatística, os classificadores Naive Bayes são uma família de "classificadores probabilísticos" simples, baseados na aplicação do teorema de Bayes com fortes pressupostos de independência entre os recursos. Eles estão entre os modelos de rede bayesianos mais simples.

## IA Activation Functions

 **ELU:** A Unidade Linear Exponencial ou seu nome amplamente conhecido ELU é uma função que tende a convergir o custo para zero mais rapidamente e produzir resultados mais precisos. Diferente de outras funções de ativação, ELU tem uma constante alfa extra que deve ser um número positivo.

 **Limiar:** Retorna 0 ou 1, muito utilizada para ativar ou desativar um neurônio.

 **Linear:** Uma função de linha reta em que a ativação é proporcional à entrada (que é a soma ponderada do neurônio).

 **ReLu:** significa Unidades Lineares Retificadas. A fórmula é aparentemente simples: max(0,z). Apesar do nome e da aparência, não é linear e oferece os mesmos benefícios do Sigmoid, mas com melhor desempenho.

 **Sigmoid:** Sigmoid recebe um valor real como entrada e produz outro valor entre 0 e 1. É fácil de trabalhar e tem todas as boas propriedades das funções de ativação: é não linear, continuamente diferenciável, monotônico e tem uma faixa de saída fixa.

 **Tanh:** Tanh comprime um número de valor real no intervalo [-1, 1]. Não é linear. Mas, ao contrário de Sigmoid, sua saída é centrada em zero. Portanto, na prática, a não linearidade tanh é sempre preferida à não linearidade sigmóide.

## Optimization

 **MSE:** Mede a média dos quadrados dos erros, ou seja, a diferença quadrática média entre os valores estimados e o que é estimado. MSE é uma função de risco, correspondendo ao valor esperado da perda de erro quadrática. O fato de o MSE ser quase sempre estritamente positivo (e não zero) é devido à aleatoriedade ou porque o estimador não leva em conta as informações que poderiam produzir uma estimativa mais precisa.